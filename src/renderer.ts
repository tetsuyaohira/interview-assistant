console.log('Interview Assistant renderer loaded');

// DOM elements
const transcriptionArea = document.getElementById('transcription') as HTMLDivElement;
const responseArea = document.getElementById('response') as HTMLDivElement;
const startBtn = document.getElementById('startBtn') as HTMLButtonElement;
const stopBtn = document.getElementById('stopBtn') as HTMLButtonElement;
const statusElement = document.getElementById('status') as HTMLSpanElement;
const envStatusElement = document.getElementById('envStatus') as HTMLSpanElement;
const selectionBtn = document.getElementById('selectionBtn') as HTMLButtonElement;
const manualInput = document.getElementById('manualInput') as HTMLTextAreaElement;
const manualSubmitBtn = document.getElementById('manualSubmitBtn') as HTMLButtonElement;
const audioDeviceSelect = document.getElementById('audioDevice') as HTMLSelectElement;

// Mock interview data
const mockTranscription = `é¢æ¥å®˜: æœ¬æ—¥ã¯ãŠå¿™ã—ã„ä¸­ãŠæ™‚é–“ã‚’ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã¾ãšè‡ªå·±ç´¹ä»‹ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

å¿œå‹Ÿè€…: ã¯ã„ã€â—‹â—‹ã¨ç”³ã—ã¾ã™ã€‚å¤§å­¦ã§ã¯æƒ…å ±å·¥å­¦ã‚’å°‚æ”»ã—ã€å’æ¥­å¾Œã¯å‰è·ã§Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹ç™ºã«3å¹´é–“å¾“äº‹ã—ã¦ã¾ã„ã‚Šã¾ã—ãŸã€‚

é¢æ¥å®˜: ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚å‰è·ã§ã¯ã©ã®ã‚ˆã†ãªæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã‚’ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã—ãŸã‹ï¼Ÿ

å¿œå‹Ÿè€…: ä¸»ã«Reactã¨Node.jsã‚’ä½¿ç”¨ã—ãŸãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯é–‹ç™ºã‚’æ‹…å½“ã—ã¦ãŠã‚Šã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯PostgreSQLã‚’ä½¿ç”¨ã—ã€AWSã§ã®ãƒ‡ãƒ—ãƒ­ã‚¤çµŒé¨“ã‚‚ã”ã–ã„ã¾ã™ã€‚

é¢æ¥å®˜: ç´ æ™´ã‚‰ã—ã„ã§ã™ã­ã€‚ã§ã¯ã€ã‚ãªãŸãŒä»Šã¾ã§é–‹ç™ºã—ãŸä¸­ã§æœ€ã‚‚å°è±¡ã«æ®‹ã£ã¦ã„ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚`;

const mockResponse = `ã“ã®è³ªå•ã¯ã€Œæœ€ã‚‚å°è±¡ã«æ®‹ã£ã¦ã„ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã«ã¤ã„ã¦èã„ã¦ã„ã‚‹ã®ã§ã€ä»¥ä¸‹ã®è¦ç´ ã‚’å«ã‚ã¦å›ç­”ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š

â€¢ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ï¼ˆä½•ã‚’é–‹ç™ºã—ãŸã‹ï¼‰
â€¢ ä½¿ç”¨ã—ãŸæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
â€¢ ã‚ãªãŸã®å½¹å‰²ã¨è²¬ä»»
â€¢ ç›´é¢ã—ãŸèª²é¡Œã¨ãã®è§£æ±ºæ–¹æ³•
â€¢ å­¦ã‚“ã ã“ã¨ã‚„æˆæœ

ä¾‹ï¼š
ã€ŒECã‚µã‚¤ãƒˆã®ãƒªãƒ‹ãƒ¥ãƒ¼ã‚¢ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒæœ€ã‚‚å°è±¡ã«æ®‹ã£ã¦ã„ã¾ã™ã€‚React + TypeScriptã§ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚’å†æ§‹ç¯‰ã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚’æ‹…å½“ã—ã¾ã—ãŸã€‚ç‰¹ã«ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿é€Ÿåº¦ã‚’40%å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®å¤§å¹…ãªæ”¹å–„ã«ã¤ãªãŒã‚Šã¾ã—ãŸã€‚ã€`;

// Audio capture state
let isRecording = false;
let audioChunks: Buffer[] = [];
let realTimeTranscription = '';

// Text selection state
let selectedText = '';
let isButtonVisible = false;

// Initialize the app
function init(): void {
    console.log('Initializing Interview Assistant...');
    
    // Display mock data
    displayTranscription(mockTranscription);
    displayResponse(mockResponse);
    
    // Enable text selection functionality
    enableTextSelection();
    
    // Setup audio capture controls
    setupAudioControls();
    
    // Setup selection button
    setupSelectionButton();
    
    // Setup manual input
    setupManualInput();
    
    // Setup IPC listeners
    setupIPCListeners();
    
    // Check environment and transcription status
    checkEnvironmentStatus();
}

function setupAudioControls(): void {
    startBtn.addEventListener('click', startRecording);
    stopBtn.addEventListener('click', stopRecording);
}

function setupSelectionButton(): void {
    // Hide button initially
    selectionBtn.style.display = 'none';
    
    // Button click handler
    selectionBtn.addEventListener('click', handleSelectionButtonClick);
}

function setupManualInput(): void {
    manualSubmitBtn.addEventListener('click', handleManualSubmit);
    
    // Enable Enter key to submit (with Ctrl/Cmd)
    manualInput.addEventListener('keydown', (event) => {
        if ((event.ctrlKey || event.metaKey) && event.key === 'Enter') {
            event.preventDefault();
            handleManualSubmit();
        }
    });
    
    // Hide button when clicking outside
    document.addEventListener('click', (event) => {
        if (!transcriptionArea.contains(event.target as Node) && 
            !selectionBtn.contains(event.target as Node)) {
            hideSelectionButton();
        }
    });
}

async function checkEnvironmentStatus(): Promise<void> {
    try {
        console.log('Checking environment status...');
        
        // Check .env file status
        const envResult = await window.electronAPI.checkEnvStatus();
        console.log('Env result:', envResult);
        updateEnvStatus(envResult.hasApiKey, envResult.message);
        
        // Wait a bit for the transcription service to initialize
        await new Promise(resolve => setTimeout(resolve, 500));
        
    } catch (error) {
        console.error('Failed to check environment status:', error);
        updateEnvStatus(false, 'ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ');
    }
}

function updateEnvStatus(hasApiKey: boolean, message: string): void {
    envStatusElement.textContent = message;
    envStatusElement.className = 'env-status';
    
    if (hasApiKey) {
        envStatusElement.classList.add('found');
    } else {
        envStatusElement.classList.add('not-found');
    }
}

async function startRecording(): Promise<void> {
    try {
        updateStatus('éŒ²éŸ³é–‹å§‹ä¸­...');
        const selectedDevice = audioDeviceSelect.value;
        console.log('Selected audio device:', selectedDevice);
        const result = await window.electronAPI.startAudioCapture(selectedDevice);
        
        if (result.success) {
            isRecording = true;
            startBtn.disabled = true;
            stopBtn.disabled = false;
            audioDeviceSelect.disabled = true; // Disable device selection while recording
            updateStatus('éŒ²éŸ³ä¸­');
            realTimeTranscription = '';
            
            // Check if transcription is ready
            const isReady = await window.electronAPI.isTranscriptionReady();
            const deviceName = selectedDevice === 'default' ? 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒã‚¤ã‚¯' : selectedDevice;
            if (isReady) {
                transcriptionArea.textContent = `éŸ³å£°ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦ã„ã¾ã™... (ãƒ‡ãƒã‚¤ã‚¹: ${deviceName})\næ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆ5ç§’é–“éš”ï¼‰`;
            } else {
                transcriptionArea.textContent = `éŸ³å£°ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦ã„ã¾ã™... (ãƒ‡ãƒã‚¤ã‚¹: ${deviceName})\n.envãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’è¨­å®šã™ã‚‹ã¨æ–‡å­—èµ·ã“ã—ãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚`;
            }
        } else {
            updateStatus('ã‚¨ãƒ©ãƒ¼: ' + result.error);
            console.error('Recording failed:', result.error);
        }
    } catch (error) {
        updateStatus('éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼');
        console.error('Start recording error:', error);
        audioDeviceSelect.disabled = false;
    }
}

async function stopRecording(): Promise<void> {
    try {
        updateStatus('éŒ²éŸ³åœæ­¢ä¸­...');
        const result = await window.electronAPI.stopAudioCapture();
        
        if (result.success) {
            isRecording = false;
            startBtn.disabled = false;
            stopBtn.disabled = true;
            audioDeviceSelect.disabled = false; // Re-enable device selection
            updateStatus('å¾…æ©Ÿä¸­');
            transcriptionArea.textContent = mockTranscription;
        } else {
            updateStatus('ã‚¨ãƒ©ãƒ¼: ' + result.error);
            console.error('Stop recording failed:', result.error);
        }
    } catch (error) {
        updateStatus('éŒ²éŸ³åœæ­¢ã‚¨ãƒ©ãƒ¼');
        console.error('Stop recording error:', error);
        audioDeviceSelect.disabled = false;
    }
}


function updateStatus(status: string): void {
    statusElement.textContent = status;
}


function setupIPCListeners(): void {
    window.electronAPI.onAudioCaptureStarted(() => {
        console.log('Audio capture started from main process');
        updateStatus('éŒ²éŸ³ä¸­');
    });
    
    window.electronAPI.onAudioCaptureStopped(() => {
        console.log('Audio capture stopped from main process');
        updateStatus('å¾…æ©Ÿä¸­');
    });
    
    window.electronAPI.onAudioData((event, data: Buffer) => {
        audioChunks.push(data);
        console.log('Received audio data chunk:', data.length, 'bytes');
    });
    
    window.electronAPI.onAudioCaptureError((event, error: string) => {
        console.error('Audio capture error from main process:', error);
        updateStatus('ã‚¨ãƒ©ãƒ¼: ' + error);
        isRecording = false;
        startBtn.disabled = false;
        stopBtn.disabled = true;
        audioDeviceSelect.disabled = false;
    });

    window.electronAPI.onTranscriptionResult((event, text: string) => {
        console.log('Received transcription:', text);
        if (text.trim()) {
            realTimeTranscription += (realTimeTranscription ? '\n\n' : '') + text.trim();
            displayTranscription(realTimeTranscription);
        }
    });

    window.electronAPI.onTranscriptionError((event, error: string) => {
        console.error('Transcription error:', error);
    });
}

function displayTranscription(text: string): void {
    transcriptionArea.textContent = text;
}

function displayResponse(text: string): void {
    responseArea.textContent = text;
}

function enableTextSelection(): void {
    transcriptionArea.addEventListener('mouseup', handleTextSelection);
    transcriptionArea.addEventListener('keyup', handleTextSelection);
}

function handleTextSelection(): void {
    const selection = window.getSelection();
    const selectedTextContent = selection?.toString().trim();
    
    if (selectedTextContent && selectedTextContent.length > 10) {
        console.log('Selected text:', selectedTextContent);
        selectedText = selectedTextContent;
        showSelectionButton();
    } else {
        hideSelectionButton();
    }
}

function showSelectionButton(): void {
    if (isButtonVisible) return;
    
    const selection = window.getSelection();
    if (!selection || selection.rangeCount === 0) return;
    
    const range = selection.getRangeAt(0);
    const rect = range.getBoundingClientRect();
    const containerRect = transcriptionArea.getBoundingClientRect();
    const paneRect = transcriptionArea.closest('.pane')?.getBoundingClientRect();
    
    if (!paneRect) return;
    
    // Position button directly below the selected text (relative to pane)
    const buttonX = Math.max(5, Math.min(
        rect.left - paneRect.left,
        paneRect.width - 180 // Button width
    ));
    const buttonY = rect.bottom - paneRect.top + 8;
    
    selectionBtn.style.left = `${buttonX}px`;
    selectionBtn.style.top = `${buttonY}px`;
    selectionBtn.style.display = 'block';
    selectionBtn.classList.add('visible');
    
    isButtonVisible = true;
    console.log('Selection button shown for text:', selectedText.substring(0, 50) + '...');
}

function hideSelectionButton(clearSelection: boolean = true): void {
    if (!isButtonVisible) return;
    
    selectionBtn.classList.remove('visible');
    selectionBtn.style.display = 'none';
    isButtonVisible = false;
    
    if (clearSelection) {
        selectedText = '';
        // Clear any text selection
        const selection = window.getSelection();
        if (selection) {
            selection.removeAllRanges();
        }
    }
}

function handleSelectionButtonClick(): void {
    // Get current selection again to make sure we have the latest
    const selection = window.getSelection();
    const currentSelectedText = selection?.toString().trim() || selectedText;
    
    if (!currentSelectedText) {
        console.warn('No text selected');
        alert('ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠã—ã¦ã‹ã‚‰ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„');
        return;
    }
    
    console.log('Generating interview response for:', currentSelectedText);
    
    // Update UI to show processing
    responseArea.textContent = `é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:\n"${currentSelectedText}"\n\né¢æ¥å›ç­”ã‚’ç”Ÿæˆä¸­...`;
    
    // Hide the selection button but keep the selection
    hideSelectionButton(false);
    
    // Generate interview response with current selection
    generateInterviewResponse(currentSelectedText, false);
}

async function handleManualSubmit(): Promise<void> {
    const inputText = manualInput.value.trim();
    
    if (!inputText) {
        alert('è³ªå•ã‚„é¢æ¥å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„');
        manualInput.focus();
        return;
    }
    
    console.log('Manual input submitted:', inputText);
    
    // Disable button during processing
    manualSubmitBtn.disabled = true;
    manualSubmitBtn.textContent = 'ç”Ÿæˆä¸­...';
    
    try {
        // Generate response
        await generateInterviewResponse(inputText, true);
    } finally {
        // Re-enable button
        manualSubmitBtn.disabled = false;
        manualSubmitBtn.textContent = 'AIå›ç­”ç”Ÿæˆ';
    }
}

async function generateInterviewResponse(text: string, isManualInput: boolean = false): Promise<void> {
    const sourceLabel = isManualInput ? 'å…¥åŠ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ' : 'é¸æŠã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ';
    
    try {
        console.log('Generating interview response for:', text);
        
        // Show initial progress
        responseArea.textContent = `${sourceLabel}:\n"${text}"\n\nğŸ”„ é¢æ¥ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ä¸­...`;
        
        // Check if Interview Assistant is available
        const isAvailable = await window.electronAPI.checkInterviewAssistantAvailability();
        if (!isAvailable) {
            responseArea.textContent = `${sourceLabel}:\n"${text}"\n\nâŒ é¢æ¥ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\n\n.envãƒ•ã‚¡ã‚¤ãƒ«ã«OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚`;
            return;
        }
        
        // Show sending progress
        responseArea.textContent = `${sourceLabel}:\n"${text}"\n\nğŸš€ é¢æ¥å›ç­”ã‚’ç”Ÿæˆä¸­...\n(3-5ç§’ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™)`;
        
        // Send to Interview Assistant
        const result = await window.electronAPI.generateInterviewResponse(text);
        
        if (result.success && result.response) {
            responseArea.textContent = `${sourceLabel}:\n"${text}"\n\nâœ… é¢æ¥å›ç­”:\n\n${result.response}`;
            
            // Clear manual input if successful and was manual input
            if (isManualInput) {
                manualInput.value = '';
            }
        } else {
            responseArea.textContent = `${sourceLabel}:\n"${text}"\n\nâŒ ã‚¨ãƒ©ãƒ¼: ${result.error || 'é¢æ¥å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ'}\n\nãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\n1. .envãƒ•ã‚¡ã‚¤ãƒ«ã«OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n2. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª\n3. OpenAI APIã‚­ãƒ¼ãŒæœ‰åŠ¹ã‹ç¢ºèª`;
        }
        
    } catch (error) {
        console.error('Error communicating with Interview Assistant:', error);
        responseArea.textContent = `${sourceLabel}:\n"${text}"\n\nâŒ é€šä¿¡ã‚¨ãƒ©ãƒ¼: ${error}\n\né¢æ¥ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã®é€šä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚`;
    }
}

// Call init when DOM is loaded
document.addEventListener('DOMContentLoaded', init);